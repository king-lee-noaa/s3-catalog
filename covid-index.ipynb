{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c481288-ba27-442c-a940-f000c16ad523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b684fb-0663-40e6-a5b0-6f777b6e9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f9ac6c0-0201-4b31-899e-c93c97ea50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "create_template = True\n",
    "create_index = True\n",
    "index_prefix = \"s3-catalog\"\n",
    "\n",
    "# None means use default profile\n",
    "profile = None\n",
    "defaultProfileName = 'covid'\n",
    "\n",
    "# Filter = { bucket: [prefix, ...], bucket: [prefix, ...], ... }\n",
    "# None means no filter\n",
    "filter = None\n",
    "# filter = {\n",
    "#    'dev-ncis-cov19-jpss': ['VIIRS_SDR_Test/Cloud_LUTs/SDRs/GITCO'],\n",
    "#    'dev-ncis-cov19-jpss-result': ['EDR-VIIRS-AOD-MATCHUP/NOAA20-MAN']\n",
    "# }\n",
    "\n",
    "# exclude storage class to rollup (to prefix):\n",
    "# None means do not rollup\n",
    "# exclude_rollup = None\n",
    "exclude_rollup = ['STANDARD']\n",
    "\n",
    "# openSearch domain\n",
    "host = \"search-s3-catalog-01-t7kxo5axijkjjfolddhnaqb3gq.us-east-1.es.amazonaws.com\"\n",
    "# host = \"search-s3-catalog-02-6qotxfn5xtynachla53iczd5qq.us-east-1.es.amazonaws.com\"\n",
    "port = 443\n",
    "auth = ('admin', 'OpenSearch1#')\n",
    "\n",
    "pageSize = 1021\n",
    "batchSize = 1021\n",
    "\n",
    "paginatorThreads = 10\n",
    "indexingThreads = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550bee1b-bc41-4ad6-b93f-be640ce41ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived\n",
    "lastIndexed = datetime.now()\n",
    "\n",
    "if profile is None:\n",
    "    session = boto3.session.Session()\n",
    "    profileName = defaultProfileName\n",
    "else:\n",
    "    session = boto3.session.Session(profile_name=profile)\n",
    "    profileName = profile\n",
    "\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "openSearch = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': port}],\n",
    "    http_compress=True,\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01244be-d39a-460c-961f-a6f667620005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index template\n",
    "def do_create_template():\n",
    "    templateName = index_prefix + '-templates'\n",
    "    openSearch.indices.put_index_template(\n",
    "      name=templateName,\n",
    "      body={\n",
    "        'index_patterns': [index_prefix + '-*'],\n",
    "        'template': {\n",
    "          'settings': {\n",
    "            'index': {\n",
    "              'number_of_shards': 1,\n",
    "              'number_of_replicas': 0\n",
    "            }\n",
    "          },\n",
    "          'mappings': {\n",
    "            \"dynamic_templates\": [\n",
    "              {\n",
    "                \"strings\": {\n",
    "                  \"mapping\": {\n",
    "                    \"type\": \"keyword\"\n",
    "                  },\n",
    "                  \"match_mapping_type\": \"string\"\n",
    "                }\n",
    "              }\n",
    "            ],\n",
    "            'properties': {\n",
    "              \"Key\": {\n",
    "                \"fields\": {\n",
    "                  \"keyword\": {\n",
    "                    \"ignore_above\": 256,\n",
    "                    \"type\": \"keyword\"\n",
    "                  }\n",
    "                },\n",
    "                \"type\": \"text\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    )\n",
    "    print('{} index template [{}] created'.format(datetime.now(), templateName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1344db73-49c5-44c7-b66b-054f2aac11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index \n",
    "def do_create_index():\n",
    "    indexName = index_prefix + '-' + profileName\n",
    "    openSearch.indices.create(index=indexName)\n",
    "    print('{} index [{}] created'.format(datetime.now(), indexName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78e4db98-8679-45d0-bd3b-20604a64d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get list of buckets present in AWS using S3 client\n",
    "def get_buckets():\n",
    "    buckets = []\n",
    "    response = s3_client.list_buckets()\n",
    "    for bucket in response['Buckets']:\n",
    "        buckets.append(bucket[\"Name\"])\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4dc7924-372a-4c4a-81c8-6c84c4fa66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_indexing(Batch):\n",
    "    actions = []\n",
    "    for document in Batch:\n",
    "        actions.append({\n",
    "            \"_op_type\": \"index\", \n",
    "            \"_index\": index_prefix + '-{}'.format(document['Profile']),\n",
    "            \"_id\": uuid.uuid5(uuid.NAMESPACE_X500, document['Key']),\n",
    "            \"_source\": document\n",
    "        })\n",
    "\n",
    "    success, failed = bulk(openSearch, actions)\n",
    "    if len(failed) > 0:\n",
    "        print('{} bulk: success={} | failed={}'.format(datetime.now(), success, failed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f964cc9-8e11-40b7-ad97-a5fa5629b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich content\n",
    "def do_enrich(Bucket, Aggregate, Content):\n",
    "    key = Content['Key']\n",
    "\n",
    "    # prefix\n",
    "    keyParts = key.split('/')\n",
    "    keyPartsLen = len(keyParts)\n",
    "    if keyPartsLen > 1:\n",
    "        prefix = keyParts[0]\n",
    "        Content.update({'Prefix1': prefix})\n",
    "        for i in range(1, keyPartsLen-1):\n",
    "            prefix = '/'.join([prefix, keyParts[i]])\n",
    "            Content.update({'Prefix{}'.format(i+1): prefix})\n",
    "    else:\n",
    "        prefix = ''\n",
    "\n",
    "    Content.update({'Prefix': prefix})\n",
    "    Content.update({'FileName': keyParts[-1]})\n",
    "    Content.update({'LastIndexed': lastIndexed})\n",
    "    Content.update({'Bucket': Bucket})\n",
    "    Content.update({'Profile': profileName})\n",
    "\n",
    "    if not exclude_rollup or Content['StorageClass'] in exclude_rollup:\n",
    "        Content['ObjectCount'] = 1\n",
    "    else:\n",
    "        if prefix in Aggregate:\n",
    "            item = Aggregate[prefix]\n",
    "            item['ObjectCount'] += 1\n",
    "            item['Size'] += Content['Size']\n",
    "            Aggregate[prefix] = item\n",
    "        else:\n",
    "            Content['ObjectCount'] = 1\n",
    "            Aggregate[prefix] = Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cbc2f6d-874a-4a17-bf24-7e3398119517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process one paginator\n",
    "def do_paginator(Bucket, Paginator):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=indexingThreads) as indexingExecutor:\n",
    "        aggregate = {}\n",
    "        indexingFutures = []\n",
    "        batch = []\n",
    "        count = 0\n",
    "\n",
    "        # index standard\n",
    "        for page in Paginator:\n",
    "            for content in page[\"Contents\"]:\n",
    "                count += 1\n",
    "                do_enrich(Bucket, aggregate, content)\n",
    "                if not exclude_rollup or content['StorageClass'] in exclude_rollup:\n",
    "                    batch.append(content)\n",
    "                    if len(batch) >= batchSize:\n",
    "                        indexingFutures.append(\n",
    "                            indexingExecutor.submit(do_indexing, batch.copy()))\n",
    "                        batch.clear()\n",
    "\n",
    "                        # if all worker threads are busy, wait til one finish\n",
    "                        if len(indexingFutures) >= indexingThreads:\n",
    "                            done, not_done = concurrent.futures.wait(\n",
    "                                indexingFutures,\n",
    "                                return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                            indexingFutures.clear()\n",
    "                            indexingFutures.extend(not_done)\n",
    "\n",
    "        print('{} [{}] has {} objects'.format(datetime.now(), Bucket, count))\n",
    "\n",
    "        # index aggregate\n",
    "        if len(aggregate) > 0:\n",
    "            for value in aggregate.values():\n",
    "                batch.append(value)\n",
    "                if len(batch) >= batchSize:\n",
    "                    indexingFutures.append(\n",
    "                        indexingExecutor.submit(do_indexing, batch.copy()))\n",
    "                    batch.clear()\n",
    "\n",
    "                    # if all worker threads are busy, wait til one finish\n",
    "                    if len(indexingFutures) >= indexingThreads:\n",
    "                        done, not_done = concurrent.futures.wait(\n",
    "                            indexingFutures,\n",
    "                            return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                        indexingFutures.clear()\n",
    "                        indexingFutures.extend(not_done)\n",
    "\n",
    "        # submit last left over batch\n",
    "        if len(batch) > 0:\n",
    "            indexingFutures.append(\n",
    "                indexingExecutor.submit(do_indexing, batch))\n",
    "\n",
    "        # wait for outstanding futures to finish\n",
    "        if len(indexingFutures) > 0:\n",
    "            done, not_done = concurrent.futures.wait(\n",
    "                indexingFutures,\n",
    "                return_when=concurrent.futures.ALL_COMPLETED)\n",
    "\n",
    "        # clean up\n",
    "        indexingExecutor.shutdown(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf858f70-5fe6-48c5-a5fa-8f2e08839255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-01 18:56:36.743486 started\n",
      "2024-03-01 18:56:37.220496 index template [s3-catalog-templates] created\n",
      "2024-03-01 18:56:37.451424 index [s3-catalog-covid] created\n",
      "2024-03-01 18:56:37.529267 [covid] has 34 buckets\n",
      "2024-03-01 18:56:37.763824 [aws-glue-scripts-716453263077-us-east-1] has 2 objects\n",
      "2024-03-01 18:56:37.787750 [aws-us-east-1-716453263077-test-application-pipe] has 3 objects\n",
      "2024-03-01 18:56:37.815763 [cdk-hnb659fds-assets-716453263077-us-east-1] has 3 objects\n",
      "2024-03-01 18:56:37.819490 [cf-templates-611xjo1ivfol-us-east-1] has 10 objects\n",
      "2024-03-01 18:56:37.821876 [cmr-files] has 6 objects\n",
      "2024-03-01 18:56:37.826001 [cid-716453263077-local] has 14 objects\n",
      "2024-03-01 18:56:37.890848 [aws-us-east-1-716453263077-class-integration-pipe] has 257 objects\n",
      "2024-03-01 18:56:38.057551 [athena-query-results-716453263077-us-east-1] has 401 objects\n",
      "2024-03-01 18:56:38.138332 [aws-athena-query-results-us-east-1-716453263077] has 306 objects\n",
      "2024-03-01 18:56:38.370007 [codepipeline-us-east-1-285230907859] has 48 objects\n",
      "2024-03-01 18:56:38.853886 [ncis-core] has 44 objects2024-03-01 18:56:38.866363 [jenkins-as-code-configuration] has 2 objects\n",
      "2024-03-01 18:56:38.930248 [ncis-covid-eks-us-east-1] has 1 objects\n",
      "\n",
      "2024-03-01 18:56:39.800041 [ncis-covid-ia-testing] has 7 objects\n",
      "2024-03-01 18:56:40.233086 [ncis-covid-nccf-pg-algorithm-builds] has 106 objects\n",
      "2024-03-01 18:56:40.663741 [ncis-dev-covid19-bt] has 22 objects\n",
      "2024-03-01 18:56:40.992050 [ncis-dev-cs-bt] has 34 objects\n",
      "2024-03-01 18:56:41.770161 [ncis-terraform] has 5 objects2024-03-01 18:56:41.784794 [ncis-fep-modis-data] has 290 objects\n",
      "\n",
      "2024-03-01 18:56:43.503315 [parallelcluster-e2ca1557272da85b-v1-do-not-delete] has 4 objects\n",
      "2024-03-01 18:56:44.099230 [ncis-covid-umd] has 1281 objects\n",
      "2024-03-01 18:56:46.706139 [science-demo-2021] has 51 objects\n",
      "2024-03-01 18:56:47.458870 [sagemaker-us-east-1-716453263077] has 150 objects\n",
      "2024-03-01 18:56:48.043808 [dev-ncis-cov19-jpss-restore] has 4381 objects\n",
      "2024-03-01 18:57:47.373397 [config-bucket-716453263077] has 30757 objects\n",
      "2024-03-01 18:58:06.707140 [dev-ncis-quarantine] has 41633 objects\n",
      "2024-03-01 19:01:49.880439 [nucaps-5-days-atms-cris-omps] has 217514 objects\n",
      "2024-03-01 19:03:04.373979 [bdp-data-ops] has 297027 objects\n",
      "2024-03-01 19:04:33.746226 [dev-ncis-cov19-jpss-inventory] has 410621 objects\n",
      "2024-03-01 20:59:50.232767 [dev-ncis-cov19-jpss-result] has 18793193 objects\n"
     ]
    }
   ],
   "source": [
    "# main logic\n",
    "print('{} started'.format(lastIndexed))\n",
    "\n",
    "if create_template:\n",
    "    do_create_template()\n",
    "\n",
    "if create_index:\n",
    "    do_create_index()\n",
    "\n",
    "paginator = s3_client.get_paginator('list_objects_v2')\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=paginatorThreads) as paginatorExecutor:\n",
    "    paginatorFutures = []\n",
    "\n",
    "    buckets = get_buckets()\n",
    "    print('{} [{}] has {} buckets'.format(datetime.now(), profileName, len(buckets)))\n",
    "\n",
    "    for bucket in buckets:\n",
    "        paginators = []\n",
    "\n",
    "        # initialize paginators for the bucket\n",
    "        if filter is None:\n",
    "            paginators.append(paginator.paginate(\n",
    "                Bucket=bucket,\n",
    "                PaginationConfig={\"PageSize\": pageSize}))\n",
    "        elif bucket in filter:\n",
    "            prefixes = filter[bucket]\n",
    "            if len(prefixes) > 0:\n",
    "                for prefix in filter[bucket]:\n",
    "                    paginators.append(paginator.paginate(\n",
    "                        Bucket=bucket,\n",
    "                        Prefix=prefix,\n",
    "                        PaginationConfig={\"PageSize\": pageSize}))\n",
    "            else:\n",
    "                paginators.append(paginator.paginate(\n",
    "                    Bucket=bucket,\n",
    "                    PaginationConfig={\"PageSize\": pageSize}))\n",
    "\n",
    "        # one thread per paginator\n",
    "        for pages in paginators:\n",
    "            paginatorFutures.append(\n",
    "                paginatorExecutor.submit(do_paginator, bucket, pages))\n",
    "\n",
    "            # if all worker threads are busy, wait til one finish\n",
    "            if len(paginatorFutures) >= paginatorThreads:\n",
    "                done, not_done = concurrent.futures.wait(\n",
    "                    paginatorFutures,\n",
    "                    return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                paginatorFutures.clear()\n",
    "                paginatorFutures.extend(not_done)\n",
    "\n",
    "    # wait for outstanding futures to finish\n",
    "    if len(paginatorFutures) > 0:\n",
    "        done, not_done = concurrent.futures.wait(\n",
    "            paginatorFutures,\n",
    "            return_when=concurrent.futures.ALL_COMPLETED)\n",
    "\n",
    "    # clean up\n",
    "    paginatorExecutor.shutdown(wait=False)\n",
    "\n",
    "# clean up\n",
    "openSearch.close()\n",
    "s3_client.close()\n",
    "\n",
    "print('{} finished'.format(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
