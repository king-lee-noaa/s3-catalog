{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c481288-ba27-442c-a940-f000c16ad523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b684fb-0663-40e6-a5b0-6f777b6e9715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import concurrent.futures\n",
    "import logging\n",
    "import uuid\n",
    "\n",
    "from datetime import datetime\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab5dd2-3187-49d6-a1a3-dc7c360d674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "                    # log to file\n",
    "                    # filename='example.log', encoding='utf-8',\n",
    "                    force=True)\n",
    "\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"opensearch\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ac6c0-0201-4b31-899e-c93c97ea50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables\n",
    "create_template = False\n",
    "create_index = False\n",
    "index_prefix = \"s3-catalog\"\n",
    "\n",
    "# None means use default profile\n",
    "profile = None\n",
    "defaultProfileName = 'covid'\n",
    "\n",
    "# Filter = { bucket: [prefix, ...], bucket: [prefix, ...], ... }\n",
    "# None means no filter\n",
    "# filter = None\n",
    "# filter = {\n",
    "#    'dev-ncis-cov19-jpss': ['VIIRS_SDR_Test/Cloud_LUTs/SDRs/GITCO'],\n",
    "#    'dev-ncis-cov19-jpss-result': ['EDR-VIIRS-AOD-MATCHUP/NOAA20-MAN']\n",
    "# }\n",
    "filter = {\n",
    "   'dev-ncis-cov19-snpp-gitco-reprocessed-v2': ['2013', '2012']\n",
    "}\n",
    "\n",
    "# exclude storage class to rollup (to prefix):\n",
    "# None means do not rollup\n",
    "# exclude_rollup = None\n",
    "exclude_rollup = ['STANDARD']\n",
    "\n",
    "# openSearch domain\n",
    "host = \"search-s3-catalog-01-t7kxo5axijkjjfolddhnaqb3gq.us-east-1.es.amazonaws.com\"\n",
    "# host = \"search-s3-catalog-02-6qotxfn5xtynachla53iczd5qq.us-east-1.es.amazonaws.com\"\n",
    "port = 443\n",
    "auth = ('admin', 'OpenSearch1#')\n",
    "\n",
    "pageSize = 1021\n",
    "batchSize = 1021\n",
    "\n",
    "paginatorThreads = 10\n",
    "indexingThreads = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bee1b-bc41-4ad6-b93f-be640ce41ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived\n",
    "lastIndexed = datetime.now()\n",
    "\n",
    "if profile is None:\n",
    "    session = boto3.session.Session()\n",
    "    profileName = defaultProfileName\n",
    "else:\n",
    "    session = boto3.session.Session(profile_name=profile)\n",
    "    profileName = profile\n",
    "\n",
    "s3_client = session.client('s3')\n",
    "\n",
    "openSearch = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': port}],\n",
    "    http_compress=True,\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_assert_hostname=False,\n",
    "    ssl_show_warn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01244be-d39a-460c-961f-a6f667620005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index template\n",
    "def do_create_template():\n",
    "    templateName = index_prefix + '-templates'\n",
    "    openSearch.indices.put_index_template(\n",
    "      name=templateName,\n",
    "      body={\n",
    "        'index_patterns': [index_prefix + '-*'],\n",
    "        'template': {\n",
    "          'settings': {\n",
    "            'index': {\n",
    "              'number_of_shards': 1,\n",
    "              'number_of_replicas': 0\n",
    "            }\n",
    "          },\n",
    "          'mappings': {\n",
    "            \"dynamic_templates\": [\n",
    "              {\n",
    "                \"strings\": {\n",
    "                  \"mapping\": {\n",
    "                    \"type\": \"keyword\"\n",
    "                  },\n",
    "                  \"match_mapping_type\": \"string\"\n",
    "                }\n",
    "              }\n",
    "            ],\n",
    "            'properties': {\n",
    "              \"Key\": {\n",
    "                \"fields\": {\n",
    "                  \"keyword\": {\n",
    "                    \"ignore_above\": 256,\n",
    "                    \"type\": \"keyword\"\n",
    "                  }\n",
    "                },\n",
    "                \"type\": \"text\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    )\n",
    "    logging.info('index template [{}] created'.format(templateName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344db73-49c5-44c7-b66b-054f2aac11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index \n",
    "def do_create_index():\n",
    "    indexName = index_prefix + '-' + profileName\n",
    "    openSearch.indices.create(index=indexName)\n",
    "    logging.info('index [{}] created'.format(indexName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4db98-8679-45d0-bd3b-20604a64d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get list of buckets present in AWS using S3 client\n",
    "def get_buckets():\n",
    "    buckets = []\n",
    "    response = s3_client.list_buckets()\n",
    "    for bucket in response['Buckets']:\n",
    "        buckets.append(bucket[\"Name\"])\n",
    "    return buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc7924-372a-4c4a-81c8-6c84c4fa66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_indexing(Batch):\n",
    "    actions = []\n",
    "    for document in Batch:\n",
    "        actions.append({\n",
    "            \"_op_type\": \"index\", \n",
    "            \"_index\": index_prefix + '-{}'.format(document['Profile']),\n",
    "            \"_id\": uuid.uuid5(uuid.NAMESPACE_X500, document['Key']),\n",
    "            \"_source\": document\n",
    "        })\n",
    "\n",
    "    success, failed = bulk(openSearch, actions)\n",
    "    if len(failed) > 0:\n",
    "        logging.info('bulk: success={} | failed={}'.format(success, failed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f964cc9-8e11-40b7-ad97-a5fa5629b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich content\n",
    "def do_enrich(Bucket, Aggregate, Content):\n",
    "    key = Content['Key']\n",
    "\n",
    "    # prefix\n",
    "    keyParts = key.split('/')\n",
    "    keyPartsLen = len(keyParts)\n",
    "    if keyPartsLen > 1:\n",
    "        prefix = keyParts[0]\n",
    "        Content.update({'Prefix1': prefix})\n",
    "        for i in range(1, keyPartsLen-1):\n",
    "            prefix = '/'.join([prefix, keyParts[i]])\n",
    "            Content.update({'Prefix{}'.format(i+1): prefix})\n",
    "    else:\n",
    "        prefix = ''\n",
    "\n",
    "    Content.update({'Prefix': prefix})\n",
    "    Content.update({'FileName': keyParts[-1]})\n",
    "    Content.update({'LastIndexed': lastIndexed})\n",
    "    Content.update({'Bucket': Bucket})\n",
    "    Content.update({'Profile': profileName})\n",
    "\n",
    "    if not exclude_rollup or Content['StorageClass'] in exclude_rollup:\n",
    "        Content['ObjectCount'] = 1\n",
    "    else:\n",
    "        if prefix in Aggregate:\n",
    "            item = Aggregate[prefix]\n",
    "            item['ObjectCount'] += 1\n",
    "            item['Size'] += Content['Size']\n",
    "            Aggregate[prefix] = item\n",
    "        else:\n",
    "            Content['ObjectCount'] = 1\n",
    "            Aggregate[prefix] = Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc2f6d-874a-4a17-bf24-7e3398119517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process one paginator\n",
    "def do_paginator(Bucket, Paginator):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=indexingThreads) as indexingExecutor:\n",
    "        aggregate = {}\n",
    "        indexingFutures = []\n",
    "        batch = []\n",
    "        count = 0\n",
    "\n",
    "        # index standard\n",
    "        for page in Paginator:\n",
    "            for content in page[\"Contents\"]:\n",
    "                count += 1\n",
    "                do_enrich(Bucket, aggregate, content)\n",
    "                if not exclude_rollup or content['StorageClass'] in exclude_rollup:\n",
    "                    batch.append(content)\n",
    "                    if len(batch) >= batchSize:\n",
    "                        indexingFutures.append(\n",
    "                            indexingExecutor.submit(do_indexing, batch.copy()))\n",
    "                        batch.clear()\n",
    "\n",
    "                        # if all worker threads are busy, wait til one finish\n",
    "                        if len(indexingFutures) >= indexingThreads:\n",
    "                            done, not_done = concurrent.futures.wait(\n",
    "                                indexingFutures,\n",
    "                                return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                            indexingFutures.clear()\n",
    "                            indexingFutures.extend(not_done)\n",
    "\n",
    "        logging.info('[{}] has {} objects'.format(Bucket, count))\n",
    "\n",
    "        # index aggregate\n",
    "        if len(aggregate) > 0:\n",
    "            for value in aggregate.values():\n",
    "                batch.append(value)\n",
    "                if len(batch) >= batchSize:\n",
    "                    indexingFutures.append(\n",
    "                        indexingExecutor.submit(do_indexing, batch.copy()))\n",
    "                    batch.clear()\n",
    "\n",
    "                    # if all worker threads are busy, wait til one finish\n",
    "                    if len(indexingFutures) >= indexingThreads:\n",
    "                        done, not_done = concurrent.futures.wait(\n",
    "                            indexingFutures,\n",
    "                            return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                        indexingFutures.clear()\n",
    "                        indexingFutures.extend(not_done)\n",
    "\n",
    "        # submit last left over batch\n",
    "        if len(batch) > 0:\n",
    "            indexingFutures.append(\n",
    "                indexingExecutor.submit(do_indexing, batch))\n",
    "\n",
    "        # wait for outstanding futures to finish\n",
    "        if len(indexingFutures) > 0:\n",
    "            done, not_done = concurrent.futures.wait(\n",
    "                indexingFutures,\n",
    "                timeout=300,\n",
    "                return_when=concurrent.futures.ALL_COMPLETED)\n",
    "\n",
    "        # clean up\n",
    "        indexingExecutor.shutdown(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf858f70-5fe6-48c5-a5fa-8f2e08839255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main logic\n",
    "logging.info('started')\n",
    "\n",
    "if create_template:\n",
    "    do_create_template()\n",
    "\n",
    "if create_index:\n",
    "    do_create_index()\n",
    "\n",
    "paginator = s3_client.get_paginator('list_objects_v2')\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=paginatorThreads) as paginatorExecutor:\n",
    "    paginatorFutures = []\n",
    "\n",
    "    buckets = get_buckets()\n",
    "    logging.info('[{}] has {} buckets'.format(profileName, len(buckets)))\n",
    "\n",
    "    for bucket in buckets:\n",
    "        paginators = []\n",
    "\n",
    "        # initialize paginators for the bucket\n",
    "        if filter is None:\n",
    "            paginators.append(paginator.paginate(\n",
    "                Bucket=bucket,\n",
    "                PaginationConfig={\"PageSize\": pageSize}))\n",
    "        elif bucket in filter:\n",
    "            prefixes = filter[bucket]\n",
    "            if len(prefixes) > 0:\n",
    "                for prefix in filter[bucket]:\n",
    "                    paginators.append(paginator.paginate(\n",
    "                        Bucket=bucket,\n",
    "                        Prefix=prefix,\n",
    "                        PaginationConfig={\"PageSize\": pageSize}))\n",
    "            else:\n",
    "                paginators.append(paginator.paginate(\n",
    "                    Bucket=bucket,\n",
    "                    PaginationConfig={\"PageSize\": pageSize}))\n",
    "\n",
    "        # one thread per paginator\n",
    "        for pages in paginators:\n",
    "            paginatorFutures.append(\n",
    "                paginatorExecutor.submit(do_paginator, bucket, pages))\n",
    "\n",
    "            # if all worker threads are busy, wait til one finish\n",
    "            if len(paginatorFutures) >= paginatorThreads:\n",
    "                done, not_done = concurrent.futures.wait(\n",
    "                    paginatorFutures,\n",
    "                    return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "                paginatorFutures.clear()\n",
    "                paginatorFutures.extend(not_done)\n",
    "\n",
    "    # wait for outstanding futures to finish\n",
    "    if len(paginatorFutures) > 0:\n",
    "        done, not_done = concurrent.futures.wait(\n",
    "            paginatorFutures,\n",
    "            return_when=concurrent.futures.ALL_COMPLETED)\n",
    "\n",
    "    # clean up\n",
    "    paginatorExecutor.shutdown(wait=False)\n",
    "\n",
    "# clean up\n",
    "openSearch.close()\n",
    "s3_client.close()\n",
    "\n",
    "logging.info('finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
